# GOLD Engine Design: Groups, Comments and Noise

With the loading of the automatas and the automata execution algorithms that where discussed in the previous chapters, the engine is almost ready.
But there is one more step that was so far left out.
Not all tokens that the Lexer produces need to be given to the parser.

In the last chapter the different symbol types where presented:
| Type Name | Field Value | Description |
|-----------|-------------|-------------|
| Non Terminal | 0 | Parsing tree nodes generated by the parser, equivalent to the non terminals in the grammar rules |
| Terminal | 1 | Symbols produced by the lexer used for input by the parser |
| Skippable | 2 | Symbols produced by the lexer that can be ignored for the parser (Whitespaces, Newlines, comments, etc.) |
| EOF | 3 | End Of File token produced by the lexer to indicate no more tokens can be shifted |
| Group Start | 4 | This symbol starts a lexical group (e.g. block comments) |
| Group End | 5 | This symbols indicates the end of a lexical group |
| Comment Line | 6 | Only in v1, this symbol indicates a line comment |

Up until now we only considered Non Terminals, as the steps of the parser and Terminals and EOF as being produced by the Lexer.
This leaves Skippable, Group Start/End and Comment Line symbols, which can be produced by the DFA, but are not of interest to the parser.

Here we will look at what those symbols do and how to handle them:

## Skippable Noise
The easiest of which is probably just the category of Skippable symbols.
As the name suggest these are noise that we can just skip.
These include things like whitespaces and newlines for most languages, but also comments.

For handling those, we just need to ammend our parsing algorithm, to just skip those when encountering them from the lexer:
```javascript
function parse(text, lalr_initial_state, dfa_initial_state):
  stack = [lalr_initial_state]
  match_pos = 0
  while not finished:
    if look_ahead is None:
      look_ahead, match_pos = dfa_match(text, match_pos, dfa_initial_state)
      if look_ahead == NoMatchFound:
        return LexerError
      // If skippable just reset to none and try again
      if look_ahead.type == Skippable:
        look_ahead = None
        continue
    parser_step = lalr_step(stack, look_ahead)
    if parser_step == ParserError:
      return SyntaxError
    if parser_step == Accept:
      return Accept
    if parser_step == Shift:
      // look_ahead was consumed, so reset
      look_ahead = None
```

## Groups
More interesting are actually the groups.
Groups work a bit differently in v1.0 as compared to v5.0.

### v1.0 Groups
In v1.0 there was only one group, which was block comments.
Basically when the lexer output a group start symbol, the engine would go through the rest of the text until the lexer would find the group end symbol, and then all of the text that was found would be disregarded.

An algorithm for this might look like this:
```javascript
function next_token(text, position, dfa_initial_state):
  token, next_pos = dfa_match(dfa_initial_state, text, position)
  if token.type != GROUP_START:
    return token, next_pos
  search_pos = next_pos
  while search_pos < text.length And
        token.type != GROUP_END:
    token, next_pos = dfa_match(dfa_initial_state, text, search_pos)
    // Charwise advance until we find end token
    search_pos += 1
  // End of group found, create new skippable token
  return Token(Skippable), next_pos
```
### v5.0 Groups
Version v5.0 generalized groups to allow for other groups to be defined.
This allows grammar writers to create non comment groups, for example multi line strings.

For this v5.0 grammars contain group records.
A group has a name, which is mostly for debugging purposes, the symbol which will be produced by the group, as well as a start and end symbol.
Besides this the group can also be configured with respect to charwise or tokenwise advance, and which kind of ending it requires.

In the example above the advance is charwise, if we would always search up next_pos, which is lokated behind the token that was matched, it would be tokenwise.

For the ending, in the example above the group is open ended, meaning when the end of the file is reached, the comment symbol will be created. If the ending mode is required to be closed, than not finding the closing symbol would result in an error.

With this in mind the algorithm above can be generalized for groups like that:
```javascript
function next_token(text, position, dfa_initial_state):
  token, next_pos = dfa_match(dfa_initial_state, text, position)
  if token.type != GROUP_START:
    return token, next_pos
  current_group = groups.find_by_start(token)
  search_pos = next_pos
  while search_pos < text.length:
    token, next_pos = dfa_match(dfa_initial_state, text, search_pos)
    if current_group.end_symbol == token:
      return current_group.symbol, next_pos
    if current_group.advance_mode == Charwise:
      search_pos += 1
    else:
      search_pos = next_pos
  if current_group.ending_mode == Closed:
    return GroupNotClosed
  else:
    return current_group.symbol, next_pos
```

So far so easy.
But another addition for v5.0 groups is that they can be nestable.
For this each group has a list of other groups that can be nested within.

For handling this, instead of just tracking current_group, a stack of all active groups must be tracked.
This makes the implementation slightly more complex:
```javascript
function next_token(text, position, dfa_initial_state):
  token, next_pos = dfa_match(dfa_initial_state, text, position)
  if token.type != GROUP_START:
    return token, next_pos
  current_groups = [groups.find_by_start(token)]
  search_pos = next_pos
  while search_pos < text.length:
    token, next_pos = dfa_match(dfa_initial_state, text, search_pos)
    if current_groups.top.end_symbol == token:
      top_group = current_groups.pop()
      if current_groups is emtpy: // last group poped
        return top_group.symbol, next_pos
      // continue for next group
      search_pos = next_pos
      continue
    if token.type == GROUP_START:
      new_group = groups.find_by_start(token)
      if current_groups.top.is_nestable_with(new_group):
        current_groups.push(new_group)
        // continue with new group
        search_pos = next_pos
        continue
    // in all other cases just advance
    if current_groups.top.advance_mode == Charwise:
      search_pos += 1
    else:
      search_pos = next_pos
  // Close all open ended groups
  while current_groups not empty:
    if current_groups.top.ending_mode == Open:
      top_group = current_groups.pop()
      if current_groups is emtpy: // last group poped
        return top_group.symbol, next_pos
  // At least one closed group on the stack
  return GroupNotClosed
```

## Comment Line
Besides block comments, v1.0 also had line comments.
While in v5.0 they are simply handled as normal groups compiled into the grammar tables, in v1.0 they had had their own functionality.

For v1.0 comment lines are started by a symbol of type Comment Line, and then the lexer would just collect all symbols up the the next newline, and output them as a comment:

```javascript
function next_token(text, position, dfa_initial_state):
  token, next_pos = dfa_match(dfa_initial_state, text, position)
  if token.type == LineComment:
    for char at pos in text from postition:
      if char == NewLine:
        break
    return Symbol(Skippable), pos
  // Other group logic
```

## Implementation Notes
### v1.0 Backwards Compatibility
It is actually quite easy to create a v5.0 parser that is also backwards compatible to v1.0 grammars.
All that is needed is to create a new group for both line comments as well as block comments.

For block comments, simply the Group Start and End symbol must be selected from the symbol records in the grammar file and a new group can be created with those as start and end.
For line comments, the start of the group is the Comment Line symbol, while the end is the already existing newline symbol:

```javascript
function create_v1_comment_groups(symbols):
  block_start = symbols.search_type(GroupStart)
  block_end = symbols.search_type(GroupEnd)
  line_start = symbols.search_type(CommentLine)
  line_end = symbols.search_name("newline")
  comment_symbol = Symbol("Comment", Skippable)

  block_group = Group("Comment Block", comment_symbol,
                      block_start, block_end,
                      Charwise, Closed)
  line_group = Group("Comment Line", comment_symbol,
                      line_start, line_end,
                      Charwise, Open)
  return block_group, line_group
```

### Line Endings
One of the issues that can arise from the group implementation for line based groups, both for the generated ones by v5.0 as well as for the upgraded ones for backward compatibility is, that the grammar might rely on the newline.

For example, in a newline based language such as Basic:
```basic
  REM Comment Line
  x = 42
```
If the newline symbol after the comment line is also considered part of the comment, then it will be discarded.
The parser can therefore not see that x is actually located in a new line, and throws an error.

To avoid this, there must be special handling in the group matching logic, that if a newline is the group closer, it will not be consumed:

```javascript
function next_token(text, position, dfa_initial_state):
  ...
  while search_pos < text.length:
    token, next_pos = dfa_match(dfa_initial_state, text, search_pos)
    if current_groups.top.end_symbol == token:
      if token == NewLine:
        next_pos -= 1; // don't consume newline char
      top_group = current_groups.pop()
      if current_groups is emtpy: // last group poped
        return top_group.symbol, next_pos
  ...
```
### Token Information
As already discussed when describing the lexer, for any real parser it makes sense to track information about the parsed tokens.
This is also true for groups.
It therefore makes sense to also keep track of where each group on the stack started and where it ended, so the text sequence can be extracted:
```javascript
function next_token(text, position, dfa_initial_state):
  ...
  // Add start information to stack
  current_groups = [(position, groups.find_by_start(token))]
  search_pos = next_pos
  while search_pos < text.length:
    token, next_pos = dfa_match(dfa_initial_state, text, search_pos)
    if current_groups.top.end_symbol == token:
      top_group = current_groups.pop()
      if current_groups is emtpy: // last group poped
        // Extract text covered by group
        return Token(top_group.symbol, text.copy(top_group.start_position, next_pos)), next_pos
      // continue for next group
      search_pos = next_pos
      continue
    if token.type == GROUP_START:
      new_group = groups.find_by_start(token)
      if current_groups.top.is_nestable_with(new_group):
        // set startpos for group on stack
        current_groups.push((search_pos, new_group))
        // continue with new group
        search_pos = next_pos
        continue
  ...
```


## Example
An example for the real typescript implementation that was developed for the GOLD Parser Tools VSCode extension (at the time of writing) is:
```typescript
export function next_token(str: string, position: number, dfa: DFAState): Token|GroupError|undefined {
  let token = dfa_match(str, position, dfa);
  if (token === undefined || token.symbol.type !== SymbolType.GROUP_START) {
    return token;
  }

  let group_stack: GroupStack = [{
    group: token.symbol.group!,
    start_position: position,
    nested_tokens: []
  }];

  let current_pos = position + token.value.length;
  while (current_pos < str.length) {
    let current_group = group_stack[group_stack.length - 1];
    token = dfa_match(str, current_pos, dfa);
    // Start of new nestable group
    if (token !== undefined &&
        token.symbol.type === SymbolType.GROUP_START &&
        current_group.group.nestable_groups.has(token.symbol.group!.name)) {
      group_stack.push({
        group: token.symbol.group!,
        start_position: current_pos,
        nested_tokens: []
      });
      // new group always starts after spanning token
      current_pos += token.value.length;
      continue;
    }
    // end of group:
    if (token !== undefined &&
        token.symbol.name === current_group.group.end_symbol.name) {
      let group_end = token.position;
      // don't consume newlines
      if (token.symbol.name.toLowerCase() !== "'newline'") {
        group_end += token.value.length;
      }
      let group_token = close_group(group_stack, group_end, str);
      if (group_stack.length === 0) {
        return group_token;
      }
      // After closing continue always after the closing token
      current_pos += token.value.length;
    }
    // For all other tokens, normal increment either charwise or tokenwise
    if (current_group.group.advance_mode === "Char" ||
        token === undefined) {
      current_pos += 1;
    } else {
      current_pos += token.value.length;
    }
  }

  // Should only be here if there are groups
  // but better safe than sorry
  token = undefined;

  // Check for still open groups
  while (group_stack.length > 0) {
    let top = group_stack[group_stack.length - 1];
    if (top.group.ending_mode === "Open") {
      // Open groups can be closed at EOF
      token = close_group(group_stack, str.length, str);
    } else { // Closed groups must be finished
      return {groups: group_stack};
    }
  }

  return token;
}

```

Which is called by the parser:
```typescript
export async function parse_string(str: string, dfa: DFAState, lalr: LRState,
                             on_token?: DFAEvent,
                             on_reduce?: LREvent,
                             on_shift?: LREvent,
                             ...args: any[]
                            ): Promise<LRParseTreeNode|LexerError|GroupError|ParserError> {
  let look_ahead: Token|undefined = undefined;
  let current_pos = 0;
  let stack = lalr_setup(lalr);

  while (current_pos <= str.length || look_ahead !== undefined) {
    if (look_ahead === undefined) {
      // Lex next token
      let tok = next_token(str, current_pos, dfa);
      if (tok === undefined) {
        return {position: current_pos};
      }
      if ("groups" in tok) { // Group Error
        return tok;
      }
      current_pos += tok.value.length;
      if (tok.symbol.type === SymbolType.SKIP_SYMBOLS) {
        continue;
      }
      look_ahead = tok;
      if (on_token !== undefined) {
        await on_token(tok, ...args);
      }
      continue;
    } // else

    let current_state = stack[stack.length-1].current_state;
    let step = lalr_step(look_ahead, stack);
  ...
```